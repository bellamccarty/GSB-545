{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "---\n",
    "title: \"Practice Activity 5: Neural Networks\"\n",
    "author: \"Isabella McCarty\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: true\n",
    "    theme: pulse\n",
    "    code-line-numbers: true\n",
    "    code-tools: true\n",
    "    self-contained: true\n",
    "execute:\n",
    "  message: false\n",
    "  warning: false\n",
    "---"
   ],
   "id": "63b6e3818c7a273e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Predicting Wine Price\n",
    "\n",
    "Please review the following site for information on our dataset of interest here: [https://www.kaggle.com/datasets/dev7halo/wine-information](https://www.kaggle.com/datasets/dev7halo/wine-information)\n",
    "\n",
    "Your goal is to use the other variables in the dataset to predict wine price. Feel free to use only a subset of the variables.\n",
    "\n",
    "### Assignment Specs\n",
    "\n",
    "- You should compare Neural Networks as we discussed this week to at least one of our previous models from this quarter.\n",
    "- A secondary goal of this assignment is to test the effects of the neural network function(s) arguments on the algorithm's performance.\n",
    "- You should explore at least 5 different sets of settings for the function inputs, and you should do your best to find values for these inputs that actually change the results of your modelling. That is, try not to run three different sets of inputs that result in the same performance. The goal here is for you to better understand how to set these input values yourself in the future. Comment on what you discover about these inputs and how they behave.\n",
    "- Additionally, I'd like you to include pictures of the network architecture for each of the neural network models you run. You may hand-draw them and insert pictures into your submitted files if you wish. You may also use software (e.g. draw.io) to create nice looking diagrams. I want you to become intimately familiar with these types of models and what they look like.\n",
    "- Your submission should be built and written with non-experts as the target audience. All of your code should still be included, but do your best to narrate your work in accessible ways.\n",
    "- Again, submit an HTML, ipynb, or Colab link. Be sure to rerun your entire notebook fresh before submitting!\n"
   ],
   "id": "25f9243b9fbb681e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Data\n",
    "\n",
    "\n",
    "In this activity, we will explore a dataset containing detailed information about wines, including attributes like country, points (rating), province, variety, and winery, among others. Our main goal is to use the available information to **predict the price** of a wine. \n",
    "\n",
    "The dataset we are using is sourced from Kaggle and has already undergone some initial cleansing (hence the file name `cleansingWine.csv`). We will perform further exploration and modeling to understand the patterns and relationships between wine characteristics and their pricing.\n",
    "\n",
    "To begin, we will load the dataset using pandas and perform a quick initial inspection.\n"
   ],
   "id": "c7ced979a1fec1e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "d5ee78f5d8ebc1a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:01:09.807600Z",
     "start_time": "2025-04-26T03:01:09.737942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read in the dataset\n",
    "wine_df = pd.read_csv(\n",
    "    'Data/cleansingWine.csv', low_memory=False\n",
    ").drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "# Display the first few rows to get a sense of the structure\n",
    "wine_df.head()"
   ],
   "id": "1a776be5c0f7e9db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id                          name      producer  nation        local1  \\\n",
       "0  137197                        Altair        Altair   Chile  Rapel Valley   \n",
       "1  137198               Altair, Sideral        Altair   Chile  Rapel Valley   \n",
       "2  137199              Baron du Val Red  Baron du Val  France           NaN   \n",
       "3  137200            Baron du Val White  Baron du Val  France           NaN   \n",
       "4  137201  Benziger, Cabernet Sauvignon      Benziger     USA    California   \n",
       "\n",
       "  local2 local3 local4          varieties1   varieties2  ...    use    abv  \\\n",
       "0    NaN    NaN    NaN  Cabernet Sauvignon    Carmenere  ...  Table  14~15   \n",
       "1    NaN    NaN    NaN  Cabernet Sauvignon       Merlot  ...  Table  14~15   \n",
       "2    NaN    NaN    NaN            Carignan     Cinsault  ...  Table  11~12   \n",
       "3    NaN    NaN    NaN            Carignan  Ugni​ blanc  ...  Table  11~12   \n",
       "4    NaN    NaN    NaN  Cabernet Sauvignon          NaN  ...  Table  13~14   \n",
       "\n",
       "  degree   sweet   acidity   body   tannin   price  year   ml  \n",
       "0  17~19  SWEET1  ACIDITY4  BODY5  TANNIN4  220000  2014  750  \n",
       "1  16~18  SWEET1  ACIDITY3  BODY4  TANNIN4  110000  2016  750  \n",
       "2  15~17  SWEET2  ACIDITY3  BODY2  TANNIN2       0     0  750  \n",
       "3   9~11  SWEET1  ACIDITY3  BODY2  TANNIN1       0     0  750  \n",
       "4  17~19  SWEET1  ACIDITY3  BODY3  TANNIN4       0  2003  750  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>producer</th>\n",
       "      <th>nation</th>\n",
       "      <th>local1</th>\n",
       "      <th>local2</th>\n",
       "      <th>local3</th>\n",
       "      <th>local4</th>\n",
       "      <th>varieties1</th>\n",
       "      <th>varieties2</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>abv</th>\n",
       "      <th>degree</th>\n",
       "      <th>sweet</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>tannin</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>ml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137197</td>\n",
       "      <td>Altair</td>\n",
       "      <td>Altair</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Rapel Valley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Carmenere</td>\n",
       "      <td>...</td>\n",
       "      <td>Table</td>\n",
       "      <td>14~15</td>\n",
       "      <td>17~19</td>\n",
       "      <td>SWEET1</td>\n",
       "      <td>ACIDITY4</td>\n",
       "      <td>BODY5</td>\n",
       "      <td>TANNIN4</td>\n",
       "      <td>220000</td>\n",
       "      <td>2014</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137198</td>\n",
       "      <td>Altair, Sideral</td>\n",
       "      <td>Altair</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Rapel Valley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Merlot</td>\n",
       "      <td>...</td>\n",
       "      <td>Table</td>\n",
       "      <td>14~15</td>\n",
       "      <td>16~18</td>\n",
       "      <td>SWEET1</td>\n",
       "      <td>ACIDITY3</td>\n",
       "      <td>BODY4</td>\n",
       "      <td>TANNIN4</td>\n",
       "      <td>110000</td>\n",
       "      <td>2016</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137199</td>\n",
       "      <td>Baron du Val Red</td>\n",
       "      <td>Baron du Val</td>\n",
       "      <td>France</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carignan</td>\n",
       "      <td>Cinsault</td>\n",
       "      <td>...</td>\n",
       "      <td>Table</td>\n",
       "      <td>11~12</td>\n",
       "      <td>15~17</td>\n",
       "      <td>SWEET2</td>\n",
       "      <td>ACIDITY3</td>\n",
       "      <td>BODY2</td>\n",
       "      <td>TANNIN2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137200</td>\n",
       "      <td>Baron du Val White</td>\n",
       "      <td>Baron du Val</td>\n",
       "      <td>France</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carignan</td>\n",
       "      <td>Ugni​ blanc</td>\n",
       "      <td>...</td>\n",
       "      <td>Table</td>\n",
       "      <td>11~12</td>\n",
       "      <td>9~11</td>\n",
       "      <td>SWEET1</td>\n",
       "      <td>ACIDITY3</td>\n",
       "      <td>BODY2</td>\n",
       "      <td>TANNIN1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137201</td>\n",
       "      <td>Benziger, Cabernet Sauvignon</td>\n",
       "      <td>Benziger</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Table</td>\n",
       "      <td>13~14</td>\n",
       "      <td>17~19</td>\n",
       "      <td>SWEET1</td>\n",
       "      <td>ACIDITY3</td>\n",
       "      <td>BODY3</td>\n",
       "      <td>TANNIN4</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:01:09.849021Z",
     "start_time": "2025-04-26T03:01:09.830707Z"
    }
   },
   "cell_type": "code",
   "source": "wine_df.info()",
   "id": "c214f23550821d17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21605 entries, 0 to 21604\n",
      "Data columns (total 31 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           21605 non-null  int64 \n",
      " 1   name         21605 non-null  object\n",
      " 2   producer     21605 non-null  object\n",
      " 3   nation       21603 non-null  object\n",
      " 4   local1       20705 non-null  object\n",
      " 5   local2       11145 non-null  object\n",
      " 6   local3       3591 non-null   object\n",
      " 7   local4       2 non-null      object\n",
      " 8   varieties1   21256 non-null  object\n",
      " 9   varieties2   7518 non-null   object\n",
      " 10  varieties3   4028 non-null   object\n",
      " 11  varieties4   1330 non-null   object\n",
      " 12  varieties5   379 non-null    object\n",
      " 13  varieties6   105 non-null    object\n",
      " 14  varieties7   31 non-null     object\n",
      " 15  varieties8   18 non-null     object\n",
      " 16  varieties9   7 non-null      object\n",
      " 17  varieties10  6 non-null      object\n",
      " 18  varieties11  5 non-null      object\n",
      " 19  varieties12  4 non-null      object\n",
      " 20  type         21547 non-null  object\n",
      " 21  use          21591 non-null  object\n",
      " 22  abv          14459 non-null  object\n",
      " 23  degree       14460 non-null  object\n",
      " 24  sweet        21603 non-null  object\n",
      " 25  acidity      21592 non-null  object\n",
      " 26  body         21592 non-null  object\n",
      " 27  tannin       21592 non-null  object\n",
      " 28  price        21605 non-null  int64 \n",
      " 29  year         21605 non-null  int64 \n",
      " 30  ml           21605 non-null  int64 \n",
      "dtypes: int64(4), object(27)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modeling",
   "id": "28c70b70895439bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our goal is to predict the price of a wine based on a subset of features from the dataset.\n",
    "\n",
    "To do this, we will:\n",
    "- Build a **baseline model** using a Bagging Regressor with a Decision Tree estimator.\n",
    "- Build several **Neural Networks** with different settings to test how changes in the architecture and hyperparameters affect performance.\n",
    "\n",
    "Our target variable is **`price`**.\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "For simplicity and clarity, we focus on the following features:\n",
    "\n",
    "- `producer`\n",
    "- `type`\n",
    "- `use`\n",
    "- `abv` (Alcohol by Volume)\n",
    "- `sweet` (Sweetness level)\n",
    "- `acidity` (Acidity level)\n",
    "- `body` (Body level)\n",
    "- `tannin` (Tannin level)\n",
    "- `year` (Vintage year)\n",
    "- `local1` (Local region)\n",
    "- `varieties1` (Grape variety)\n",
    "\n",
    "These features were chosen because they are intuitively related to wine pricing and were relatively clean after preprocessing. Adding `local1` and `varieties1` helped capture more variation in wine characteristics, leading to improved model performance."
   ],
   "id": "a6f577a7fc21ab03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preparing Feature Sets",
   "id": "21e34bd26c8578"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:01:09.955404Z",
     "start_time": "2025-04-26T03:01:09.914161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select only the columns of interest\n",
    "features = ['producer', 'local1', 'varieties1', 'type', 'use', 'abv', 'sweet', 'acidity', 'body', 'tannin', 'year']\n",
    "target = 'price'\n",
    "\n",
    "# Make a copy of the working data\n",
    "model_data = wine_df[features + [target]].copy()\n",
    "\n",
    "# Drop any rows with missing values\n",
    "model_data = model_data.dropna()\n",
    "\n",
    "# Keep only rows where price is greater than 0\n",
    "model_data = model_data[model_data['price'] > 0]\n",
    "\n",
    "# Convert features to appropriate numeric types\n",
    "def clean_range(value):\n",
    "    \"\"\" Helper function to clean values like '14~15' into an average \"\"\"\n",
    "    if isinstance(value, str) and '~' in value:\n",
    "        low, high = value.split('~')\n",
    "        return (float(low) + float(high)) / 2\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for col in ['abv', 'year']:\n",
    "    model_data[col] = model_data[col].apply(clean_range)\n",
    "\n",
    "# Convert categorical columns like 'sweet', 'acidity', 'body', 'tannin'\n",
    "# These are text codes like 'SWEET1', so we extract the number\n",
    "def extract_number(value):\n",
    "    \"\"\" Helper to pull numbers out of text labels \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return int(''.join(filter(str.isdigit, value)))\n",
    "    return None\n",
    "\n",
    "for col in ['sweet', 'acidity', 'body', 'tannin']:\n",
    "    model_data[col] = model_data[col].apply(extract_number)\n",
    "\n",
    "# Drop again any rows with missing values after cleaning\n",
    "model_data = model_data.dropna()\n",
    "\n",
    "# Separate X and y\n",
    "X = model_data[features]\n",
    "y = model_data[target]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ],
   "id": "ecffdcf73f122031",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bagging\n",
    "\n",
    "To build a strong baseline for comparison against our Neural Networks, we first train a Bagging Regressor. Bagging reduces variance by averaging predictions from multiple decision trees trained on different subsets of the data. We tune key hyperparameters like the number of estimators and tree depth using GridSearchCV. The final model’s MSE and R² scores will provide a benchmark for evaluating the performance of our Neural Networks.\n"
   ],
   "id": "b75d8da6241c740c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:01:25.437255Z",
     "start_time": "2025-04-26T03:01:09.982380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Categorical features to OneHotEncode\n",
    "categorical_features = ['producer', 'local1', 'varieties1', 'type', 'use']\n",
    "categorical_transformer = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
    "\n",
    "# Preprocessor\n",
    "tree_preprocessor = ColumnTransformer(\n",
    "    transformers=[('cat', categorical_transformer, categorical_features)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Full pipeline: preprocessing + bagging\n",
    "pipe_tree = Pipeline(steps=[\n",
    "    ('preprocessor', tree_preprocessor),\n",
    "    ('regressor', BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(random_state=123),\n",
    "        random_state=123\n",
    "    )),\n",
    "])\n",
    "\n",
    "# Parameter grid for grid search\n",
    "param_grid_tree = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__estimator__max_depth': [3, 5, 7],\n",
    "    'regressor__estimator__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(pipe_tree, param_grid_tree, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_tree = grid_search.best_estimator_\n",
    "y_pred = best_tree.predict(X_test)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Bagging MSE: {mean_squared_error(y_test, y_pred):.2f}\")\n",
    "print(f\"Bagging R²: {r2_score(y_test, y_pred):.3f}\")"
   ],
   "id": "a160a7ca8433733c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'regressor__estimator__max_depth': 7, 'regressor__estimator__min_samples_split': 2, 'regressor__n_estimators': 100}\n",
      "Bagging MSE: 37368762858.38\n",
      "Bagging R²: 0.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isabe\\PycharmProjects\\GSB-545\\gsb545env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Bagging Regressor achieved a Mean Squared Error (MSE) of approximately 37.37 billion and an R² score of 0.392 on the test set. This indicates that the model explains about 39% of the variance in wine prices — a respectable performance given the complexity and variability in wine pricing. By tuning hyperparameters like the number of estimators and tree depth, we were able to strengthen the model’s ability to generalize beyond the training data. While not perfect, Bagging provided a strong baseline for comparing the more complex Neural Network models.",
   "id": "949cf27af2a7ef54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5 Neural Networks for Wine Price Prediction\n",
    "\n",
    "To explore how Neural Networks perform on our wine price prediction task, we can design five different models, each with a unique architecture or training setting. The goal was to better understand how specific design choices—like the number of neurons, depth of the network, activation functions, and regularization techniques—affect the model’s accuracy and generalization. Each model builds on the one before it, allowing us to observe the effects of increasing complexity or adding stabilization techniques. For each model, we report the test performance and provide a visual diagram of its architecture. \n",
    "\n",
    "#### Model 1: Small Simple Network\n",
    "![Model 1](Diagrams/Model1.jpg)\n",
    "\n",
    "#### Model 2: More Neurons\n",
    "![Model 2](Diagrams/Model2.jpg)\n",
    "\n",
    "#### Model 3: Deeper Network\n",
    "![Model 3](Diagrams/Model3.jpg)\n",
    "\n",
    "#### Model 4: Tanh Activation\n",
    "![Model 4](Diagrams/Model4.jpg)\n",
    "\n",
    "#### Model 5: Dropout Regularization\n",
    "![Model 5](Diagrams/Model5.jpg)\n",
    "\n",
    "___"
   ],
   "id": "eb297d715b47d8d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:01:25.526294Z",
     "start_time": "2025-04-26T03:01:25.498290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 2. Preprocessing: scale numeric features, one-hot encode categoricals\n",
    "numeric_features = ['abv', 'sweet', 'acidity', 'body', 'tannin', 'year']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the preprocessor\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "# 3. Get input shape for model\n",
    "input_shape = X_train_prep.shape[1]\n",
    "\n",
    "# Function to compile, train, and evaluate a model\n",
    "def build_and_train(model, model_name, optimizer='adam', epochs=50, batch_size=32):\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    history = model.fit(\n",
    "        X_train_prep, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    y_pred = model.predict(X_test_prep).flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{model_name} | Epochs: {epochs} | Batch Size: {batch_size}\")\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test MAE: {mae:.2f}\")\n",
    "    print(f\"Test R²: {r2:.3f}\")\n",
    "    print(\"-\" * 40)\n",
    "    return mse, mae, r2\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def grid_search_nn(model_architecture_fn, model_name, epochs_list, batch_sizes_list, learning_rates_list=None):\n",
    "    results = []\n",
    "    \n",
    "    # Handle learning rate (optional)\n",
    "    if learning_rates_list is None:\n",
    "        learning_rates_list = [0.001]\n",
    "    \n",
    "    for epochs, batch_size, lr in product(epochs_list, batch_sizes_list, learning_rates_list):\n",
    "        # Rebuild model fresh each time\n",
    "        model = model_architecture_fn()\n",
    "        \n",
    "        # Custom optimizer with given learning rate\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        mse, mae, r2 = build_and_train(\n",
    "            model,\n",
    "            model_name=f\"{model_name} (epochs={epochs}, batch={batch_size}, lr={lr})\",\n",
    "            optimizer=optimizer,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Epochs\": epochs,\n",
    "            \"Batch Size\": batch_size,\n",
    "            \"Learning Rate\": lr,\n",
    "            \"MSE\": mse,\n",
    "            \"MAE\": mae,\n",
    "            \"R2\": r2\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)"
   ],
   "id": "fd0e627cbb873fe6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isabe\\PycharmProjects\\GSB-545\\gsb545env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:01:25.632996Z",
     "start_time": "2025-04-26T03:01:25.628012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model 1: Small Simple Network\n",
    "def build_model_1():\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Model 2: More Neurons\n",
    "def build_model_2():\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Model 3: Deeper Network\n",
    "def build_model_3():\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Model 4: Different Activation Function (tanh)\n",
    "def build_model_4():\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(32, activation='tanh'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Model 5: Dropout Regularization\n",
    "def build_model_5():\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Model 6: Improved Network with Batch Normalization\n",
    "def build_model_6():\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])"
   ],
   "id": "88b52fbc4cd1d22c",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:01:25.639060Z",
     "start_time": "2025-04-26T03:01:25.636007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define hyperparameter settings\n",
    "epochs_list = [50, 100]\n",
    "batch_sizes_list = [16, 32]\n",
    "learning_rates_list = [0.001] "
   ],
   "id": "391475c2613e49e7",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model 1: Small Simple Network (Baseline)\n",
    "Architecture: Input → Dense(16, relu) → Output(1)"
   ],
   "id": "7a445f4a60b0e73b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:03:06.793277Z",
     "start_time": "2025-04-26T03:01:25.694298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_model1 = grid_search_nn(\n",
    "    model_architecture_fn=build_model_1,\n",
    "    model_name=\"Model 1: Small Simple Network\",\n",
    "    epochs_list=epochs_list,\n",
    "    batch_sizes_list=batch_sizes_list,\n",
    "    learning_rates_list=learning_rates_list\n",
    ")"
   ],
   "id": "df18eda46c30fb12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 1: Small Simple Network (epochs=50, batch=16, lr=0.001) | Epochs: 50 | Batch Size: 16\n",
      "Test MSE: 78538915840.00\n",
      "Test MAE: 132266.70\n",
      "Test R²: -0.278\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 1: Small Simple Network (epochs=50, batch=32, lr=0.001) | Epochs: 50 | Batch Size: 32\n",
      "Test MSE: 79675752448.00\n",
      "Test MAE: 135753.27\n",
      "Test R²: -0.296\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 1: Small Simple Network (epochs=100, batch=16, lr=0.001) | Epochs: 100 | Batch Size: 16\n",
      "Test MSE: 70710386688.00\n",
      "Test MAE: 107798.47\n",
      "Test R²: -0.151\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 1: Small Simple Network (epochs=100, batch=32, lr=0.001) | Epochs: 100 | Batch Size: 32\n",
      "Test MSE: 77643071488.00\n",
      "Test MAE: 129584.01\n",
      "Test R²: -0.263\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The first Neural Network used a simple architecture with just one hidden layer of 16 neurons. Across different training settings, the best result was achieved with 100 epochs and a batch size of 16, but the model still resulted in a negative R² score. This indicates that the model performed worse than simply predicting the average price for every wine. The small size of the network limited its ability to capture complex patterns in the data.",
   "id": "70d64dcc0476647"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Model 2: More Neurons (Capacity Test)\n",
    "Architecture: Input → Dense(64, relu) → Output(1)"
   ],
   "id": "29e03b394260d3b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:05:17.101149Z",
     "start_time": "2025-04-26T03:03:06.851305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_model2 = grid_search_nn(\n",
    "    model_architecture_fn=build_model_2,\n",
    "    model_name=\"Model 2: More Neurons\",\n",
    "    epochs_list=epochs_list,\n",
    "    batch_sizes_list=batch_sizes_list,\n",
    "    learning_rates_list=learning_rates_list\n",
    ")"
   ],
   "id": "c2f391f38510241d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 2: More Neurons (epochs=50, batch=16, lr=0.001) | Epochs: 50 | Batch Size: 16\n",
      "Test MSE: 69099921408.00\n",
      "Test MAE: 103644.16\n",
      "Test R²: -0.124\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 2: More Neurons (epochs=50, batch=32, lr=0.001) | Epochs: 50 | Batch Size: 32\n",
      "Test MSE: 76046262272.00\n",
      "Test MAE: 124582.50\n",
      "Test R²: -0.237\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 2: More Neurons (epochs=100, batch=16, lr=0.001) | Epochs: 100 | Batch Size: 16\n",
      "Test MSE: 60545134592.00\n",
      "Test MAE: 92502.96\n",
      "Test R²: 0.015\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 2: More Neurons (epochs=100, batch=32, lr=0.001) | Epochs: 100 | Batch Size: 32\n",
      "Test MSE: 67547275264.00\n",
      "Test MAE: 100267.10\n",
      "Test R²: -0.099\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this model, we increased the number of neurons to 64 in a single hidden layer. Although adding more neurons slightly reduced the Mean Absolute Error (MAE), the R² score remained mostly negative. This shows that simply making the network wider, without adding depth or other improvements, was not enough to meaningfully capture the complexity of wine pricing.",
   "id": "c5f2ccb99bcb5530"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model 3: Deeper Network (More Layers)\n",
    "Architecture: Input → Dense(32, relu) → Dense(16, relu) → Output(1)"
   ],
   "id": "c9feea580b01d3c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:07:14.865478Z",
     "start_time": "2025-04-26T03:05:17.173160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_model3 = grid_search_nn(\n",
    "    model_architecture_fn=build_model_3,\n",
    "    model_name=\"Model 3: Deeper Network\",\n",
    "    epochs_list=epochs_list,\n",
    "    batch_sizes_list=batch_sizes_list,\n",
    "    learning_rates_list=learning_rates_list\n",
    ")"
   ],
   "id": "1f949437681c7a49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 3: Deeper Network (epochs=50, batch=16, lr=0.001) | Epochs: 50 | Batch Size: 16\n",
      "Test MSE: 49275625472.00\n",
      "Test MAE: 91354.25\n",
      "Test R²: 0.198\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 3: Deeper Network (epochs=50, batch=32, lr=0.001) | Epochs: 50 | Batch Size: 32\n",
      "Test MSE: 51513749504.00\n",
      "Test MAE: 95582.19\n",
      "Test R²: 0.162\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 3: Deeper Network (epochs=100, batch=16, lr=0.001) | Epochs: 100 | Batch Size: 16\n",
      "Test MSE: 45317394432.00\n",
      "Test MAE: 86743.20\n",
      "Test R²: 0.263\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 3: Deeper Network (epochs=100, batch=32, lr=0.001) | Epochs: 100 | Batch Size: 32\n",
      "Test MSE: 49432190976.00\n",
      "Test MAE: 92049.80\n",
      "Test R²: 0.196\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This model added a second hidden layer (32 neurons → 16 neurons). Adding depth led to a significant improvement: we achieved a positive R² score for the first time, meaning the model was better than simply guessing the average. The best performance came from training with 100 epochs and a batch size of 16, showing that giving the model more time to learn and smaller batch updates helped it generalize better.",
   "id": "1a14de629169dc0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model 4: Different Activation (tanh)\n",
    "Architecture: Input → Dense(32, tanh) → Output(1)"
   ],
   "id": "58a23a4bc3d795cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:21:10.990774Z",
     "start_time": "2025-04-26T03:19:20.487277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_model4 = grid_search_nn(\n",
    "    model_architecture_fn=build_model_4,\n",
    "    model_name=\"Model 4: Tanh Activation\",\n",
    "    epochs_list=epochs_list,\n",
    "    batch_sizes_list=batch_sizes_list,\n",
    "    learning_rates_list=learning_rates_list\n",
    ")"
   ],
   "id": "5253f227760932d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 4: Tanh Activation (epochs=50, batch=16, lr=0.001) | Epochs: 50 | Batch Size: 16\n",
      "Test MSE: 80635559936.00\n",
      "Test MAE: 138475.80\n",
      "Test R²: -0.312\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 4: Tanh Activation (epochs=50, batch=32, lr=0.001) | Epochs: 50 | Batch Size: 32\n",
      "Test MSE: 80699146240.00\n",
      "Test MAE: 138705.20\n",
      "Test R²: -0.313\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 4: Tanh Activation (epochs=100, batch=16, lr=0.001) | Epochs: 100 | Batch Size: 16\n",
      "Test MSE: 80501809152.00\n",
      "Test MAE: 137992.02\n",
      "Test R²: -0.310\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 4: Tanh Activation (epochs=100, batch=32, lr=0.001) | Epochs: 100 | Batch Size: 32\n",
      "Test MSE: 80629530624.00\n",
      "Test MAE: 138454.02\n",
      "Test R²: -0.312\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We replaced the ReLU activation with tanh in the hidden layer. Across all training settings, this model consistently performed poorly, with high error rates and a negative R². This suggests that ReLU activation was better suited for this dataset, likely because ReLU handles wide-ranging numeric inputs without saturation issues that tanh sometimes suffers from.",
   "id": "abca9640e500ad2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model 5: Add Dropout (Regularization)\n",
    "Architecture: Input → Dense(32, relu) → Dropout(0.3) → Dense(16, relu) → Output(1)"
   ],
   "id": "5d5f557c6f548631"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:24:56.003814Z",
     "start_time": "2025-04-26T03:23:03.801492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_model5 = grid_search_nn(\n",
    "    model_architecture_fn=build_model_5,\n",
    "    model_name=\"Model 5: Dropout Regularization\",\n",
    "    epochs_list=epochs_list,\n",
    "    batch_sizes_list=batch_sizes_list,\n",
    "    learning_rates_list=learning_rates_list\n",
    ")"
   ],
   "id": "e9c6feb9f01b4278",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 5: Dropout Regularization (epochs=50, batch=16, lr=0.001) | Epochs: 50 | Batch Size: 16\n",
      "Test MSE: 49716576256.00\n",
      "Test MAE: 91169.36\n",
      "Test R²: 0.191\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 5: Dropout Regularization (epochs=50, batch=32, lr=0.001) | Epochs: 50 | Batch Size: 32\n",
      "Test MSE: 52386275328.00\n",
      "Test MAE: 96110.25\n",
      "Test R²: 0.148\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 5: Dropout Regularization (epochs=100, batch=16, lr=0.001) | Epochs: 100 | Batch Size: 16\n",
      "Test MSE: 45954666496.00\n",
      "Test MAE: 86285.38\n",
      "Test R²: 0.252\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 5: Dropout Regularization (epochs=100, batch=32, lr=0.001) | Epochs: 100 | Batch Size: 32\n",
      "Test MSE: 47438626816.00\n",
      "Test MAE: 88526.82\n",
      "Test R²: 0.228\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This network introduced a Dropout layer (30%) to help prevent overfitting. The model achieved stable but slightly lower performance compared to the deeper network without dropout. Dropout helped regularize the network and made it less prone to memorizing the training data, but it slightly limited the model’s ability to fully fit the data. Still, with 100 epochs and a batch size of 16, it achieved a reasonably strong positive R².",
   "id": "ea02afbd306729a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Improving Upon Our Best NN\n",
    "\n",
    "With help from GeeksForGeeks: https://www.geeksforgeeks.org/what-is-batch-normalization-in-deep-learning/\n",
    "\n",
    "To improve upon our best Neural Network so far, we built a new, slightly larger model with two hidden layers. The first hidden layer has 64 neurons, followed by Batch Normalization to help stabilize and speed up training. The second hidden layer has 32 neurons. We also increased the number of training epochs to 100 to give the model more time to learn the complex relationships in the data, and we used a smaller learning rate (0.001) for finer adjustments during training.\n",
    "\n",
    "#### Model 6: Improved Network with Batch Norm\n",
    "![Model 6](Diagrams/Model6.jpg)"
   ],
   "id": "6be62a6663e67eb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:27:14.300531Z",
     "start_time": "2025-04-26T03:24:56.003814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_model6 = grid_search_nn(\n",
    "    model_architecture_fn=build_model_6,\n",
    "    model_name=\"Model 6: Improved Network with Batch Norm\",\n",
    "    epochs_list=epochs_list,\n",
    "    batch_sizes_list=batch_sizes_list,\n",
    "    learning_rates_list=learning_rates_list\n",
    ")"
   ],
   "id": "c7a684b93ac644b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 6: Improved Network with Batch Norm (epochs=50, batch=16, lr=0.001) | Epochs: 50 | Batch Size: 16\n",
      "Test MSE: 44054331392.00\n",
      "Test MAE: 78002.59\n",
      "Test R²: 0.283\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 6: Improved Network with Batch Norm (epochs=50, batch=32, lr=0.001) | Epochs: 50 | Batch Size: 32\n",
      "Test MSE: 38779949056.00\n",
      "Test MAE: 83923.19\n",
      "Test R²: 0.369\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 6: Improved Network with Batch Norm (epochs=100, batch=16, lr=0.001) | Epochs: 100 | Batch Size: 16\n",
      "Test MSE: 47651987456.00\n",
      "Test MAE: 84262.73\n",
      "Test R²: 0.225\n",
      "----------------------------------------\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Model 6: Improved Network with Batch Norm (epochs=100, batch=32, lr=0.001) | Epochs: 100 | Batch Size: 32\n",
      "Test MSE: 38878797824.00\n",
      "Test MAE: 84407.50\n",
      "Test R²: 0.367\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For our best model, we expanded the network with two hidden layers (64 neurons → 32 neurons) and added Batch Normalization after the first layer. This greatly helped the model stabilize during training and reduced internal covariate shifts. Model 6 achieved the best results overall, with the highest R² score of 0.369, particularly when trained for 50 epochs with a batch size of 32. This shows that thoughtful architectural changes, combined with proper training settings, can lead Neural Networks to perform competitively even compared to ensemble methods like Bagging.",
   "id": "a63062e31e9ebd9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this activity, we explored different ways to predict wine prices using machine learning models, starting with a Bagging Regressor as a strong baseline and then developing six different Neural Network architectures. Each neural network was built with the goal of understanding how changing specific design choices—like the number of neurons, depth of layers, activation functions, dropout regularization, and batch normalization—impacts model performance. Our final and best-performing Neural Network (Model 6), which included Batch Normalization and two hidden layers, achieved an R² score of 0.369, coming close to our Bagging model's R² of 0.392. This demonstrates that, with thoughtful design and tuning, Neural Networks can be strong contenders even when compared to traditional ensemble methods. \n"
   ],
   "id": "50bc0d5654ded2e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
